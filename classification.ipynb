{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify using reservoir causality data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load computed causality matrices\n",
    "con_files = [f for f in glob.glob(\"data/sub-CON*.npy\")]\n",
    "pat_files = [f for f in glob.glob(\"data/sub-PAT*.npy\")]\n",
    "\n",
    "# filter out excluded subjects\n",
    "excluded = [\"PAT08\", \"CON03\", \"PAT03\", \"PAT11\"]\n",
    "con_files = [f for f in con_files if f[9:14] not in excluded]\n",
    "pat_files = [f for f in pat_files if f[9:14] not in excluded]\n",
    "\n",
    "# transform for LDA\n",
    "con_causalities = np.array([np.load(f) for f in con_files])\n",
    "pat_causalities = np.array([np.load(f) for f in pat_files])\n",
    "con_flattened = con_causalities.reshape((con_causalities.shape[0], -1))\n",
    "pat_flattened = pat_causalities.reshape((pat_causalities.shape[0], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# causality = np.load(\"data/sub-PAT14_ses-preop_task-rest_space-MNI152NLin2009cAsym_atlas-Gordon_desc-timeseries_bold_causality_matrix.npy\")\n",
    "# plt.figure(figsize=(10,10))\n",
    "# plt.imshow(causality, cmap='hot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1. 0. 0. 1. 1.]\n",
      "0.4444444444444444\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 1. 1. 1. 0. 0. 0. 1. 1.]\n",
      "\n",
      "LDA robustness:  0.4070992246057945\n"
     ]
    }
   ],
   "source": [
    "# perform LDA\n",
    "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "# use SVM\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# lda = LinearDiscriminantAnalysis()\n",
    "svc = SVC(kernel='linear', C=1.0, probability=True)\n",
    "\n",
    "\n",
    "_xs = np.concatenate((con_flattened, pat_flattened))\n",
    "_ys = np.concatenate((np.zeros(con_flattened.shape[0]), np.ones(pat_flattened.shape[0])))\n",
    "# replace nans in _xs with 0\n",
    "_xs[np.isnan(_xs)] = 0\n",
    "\n",
    "# 2-fold cross-validation (randomized)\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "ss = ShuffleSplit(n_splits=2, test_size=0.5, random_state=0)\n",
    "\n",
    "lda_coefs = []\n",
    "for train_index, test_index in ss.split(_xs):\n",
    "    X_train, X_test = _xs[train_index], _xs[test_index]\n",
    "    y_train, y_test = _ys[train_index], _ys[test_index]\n",
    "    lda.fit(X_train, y_train)\n",
    "    print(lda.score(X_test, y_test))\n",
    "    y_pred = lda.predict(X_test)\n",
    "    print(y_pred) \n",
    "    print(y_test)\n",
    "\n",
    "    # coefs = lda.coef_.reshape((con_causalities.shape[1], con_causalities.shape[2]))\n",
    "    # plt.figure(figsize=(10,10))\n",
    "    # plt.imshow(coefs, cmap='hot')\n",
    "    lda_coefs.append(lda.coef_)\n",
    "\n",
    "# correlation between coefficients\n",
    "lda_robustness = np.corrcoef(lda_coefs[0], lda_coefs[1])[0, 1]\n",
    "print(\"\\nLDA robustness: \", lda_robustness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coefs = lda.coef_.reshape((con_causalities.shape[1], con_causalities.shape[2]))\n",
    "# plt.figure(figsize=(10,10))\n",
    "# plt.imshow(coefs, cmap='hot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify using region correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_fmri_files = [f[:-21] + \".tsv\" for f in con_files]\n",
    "pat_fmri_files = [f[:-21] + \".tsv\" for f in pat_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_corr_matrices = []\n",
    "for filename in con_fmri_files:\n",
    "    df = pd.read_csv(filename, header=None)\n",
    "    data = np.array(df)\n",
    "    correlation_matrix = np.corrcoef(data.T)\n",
    "    con_corr_matrices.append(correlation_matrix)\n",
    "pat_corr_matrices = []\n",
    "for filename in pat_fmri_files:\n",
    "    df = pd.read_csv(filename, header=None)\n",
    "    data = np.array(df)\n",
    "    correlation_matrix = np.corrcoef(data.T)\n",
    "    pat_corr_matrices.append(correlation_matrix)\n",
    "\n",
    "# flatten for LDA\n",
    "con_correlations = np.array(con_corr_matrices)\n",
    "pat_correlations = np.array(pat_corr_matrices)\n",
    "con_flattened = con_causalities.reshape((con_correlations.shape[0], -1))\n",
    "pat_flattened = pat_causalities.reshape((pat_correlations.shape[0], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score:  0.5555555555555556\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[1. 0. 0. 0. 0. 1. 0. 1. 1.]\n",
      "score:  0.4444444444444444\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[1. 1. 1. 0. 1. 0. 1. 0. 0.]\n",
      "\n",
      "LDA robustness:  0.1909680682914709\n"
     ]
    }
   ],
   "source": [
    "# perform LDA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "_xs = np.concatenate((con_flattened, pat_flattened))\n",
    "_ys = np.concatenate((np.zeros(con_flattened.shape[0]), np.ones(pat_flattened.shape[0])))\n",
    "# replace nans in _xs with 0\n",
    "_xs[np.isnan(_xs)] = 0\n",
    "\n",
    "# 2-fold cross-validation (randomized)\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "ss = ShuffleSplit(n_splits=2, test_size=0.5, random_state=4)\n",
    "\n",
    "lda_coefs = []\n",
    "for train_index, test_index in ss.split(_xs):\n",
    "    X_train, X_test = _xs[train_index], _xs[test_index]\n",
    "    y_train, y_test = _ys[train_index], _ys[test_index]\n",
    "    lda.fit(X_train, y_train)\n",
    "    print(\"score: \", lda.score(X_test, y_test))\n",
    "    y_pred = lda.predict(X_test)\n",
    "    print(y_pred) \n",
    "    print(y_test)\n",
    "\n",
    "    # coefs = lda.coef_.reshape((con_causalities.shape[1], con_causalities.shape[2]))\n",
    "    # plt.figure(figsize=(10,10))\n",
    "    # plt.imshow(coefs, cmap='hot')\n",
    "    lda_coefs.append(lda.coef_)\n",
    "\n",
    "# correlation between coefficients\n",
    "lda_robustness = np.corrcoef(lda_coefs[0], lda_coefs[1])[0, 1]\n",
    "print(\"\\nLDA robustness: \", lda_robustness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
